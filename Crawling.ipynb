{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawling.ipynb",
      "provenance": [],
      "mount_file_id": "1GTrgUgKQUn1zT4ssasX57UmBGlc_sAIY",
      "authorship_tag": "ABX9TyPS3A9SNrgyorMO8AXt8nUZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csgm2328/Emily/blob/master/Crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpNMGvX2TL_5"
      },
      "source": [
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as pdr #data 받아오는 api 라이브러리\n",
        "from pandas import json_normalize\n",
        "import numpy as np\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import requests\n",
        "import datetime as dt\n",
        "from bs4 import BeautifulSoup\n",
        "import traceback\n",
        "import os\n",
        "from keras import metrics\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "plt.style.use('bmh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoLdrTqN2oDe"
      },
      "source": [
        "#Request URL API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwrNibU2EQE4"
      },
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/KOSDAQ200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF2eLbMA-aUp"
      },
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "stock_data = pd.read_csv('KOSDAQ200.csv', encoding='CP949')\n",
        "stock_data.종목코드 = stock_data.종목코드.map('{:06d}'.format) #이걸 해줘야 6자리 고정됨\n",
        "stock_data = stock_data.rename(columns={'회사명':'name', '종목코드':'code'})\n",
        "\n",
        "for idx,it in stock_data.iterrows():\n",
        "  Training_start = time.time()\n",
        "  filename = it[0] + ' ' + it[1] +'.csv'\n",
        "  item_name = it[1]\n",
        "  code = it[0]\n",
        "  print(code, item_name)\n",
        "    \n",
        "  \n",
        "  url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)\n",
        "  print(\"요청 URL = {}\".format(url))\n",
        "  res = requests.get(url)\n",
        "  c = 0\n",
        "  while res.reason == 'Internal Server Error':\n",
        "    if c > 60:\n",
        "      break;\n",
        "    time.sleep(10)\n",
        "    res = requests.get(url)\n",
        "    c+=1\n",
        "\n",
        "  res.encoding = 'utf-8'\n",
        "  print(res)\n",
        "  # 일자 데이터를 담을 df라는 DataFrame 정의 \n",
        "  df = pd.DataFrame() \n",
        "\n",
        "  #페이지 크롤링 = 마지막 페이지 얻기\n",
        "  soup = BeautifulSoup(res.text, 'lxml')\n",
        "  el_table_navi = soup.find(\"table\", class_=\"Nnavi\")\n",
        "  el_td_last = el_table_navi.find(\"td\", class_=\"pgRR\")\n",
        "  pg_last = el_td_last.a.get('href').rsplit('&')[1]\n",
        "  pg_last = pg_last.split('=')[1]\n",
        "  pg_last = int(pg_last)\n",
        "\n",
        "  if pg_last > 200:\n",
        "    pg_last = 200 #8년동안 데이터\n",
        "  print('마지막 쪽:', pg_last)\n",
        "\n",
        "  for page in range(1, pg_last): #2170 items/124초 소요\n",
        "    pg_url = '{url}&page={page}'.format(url=url, page=page) \n",
        "    df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True) \n",
        "\n",
        "  df = df.dropna() \n",
        "\n",
        "  #csv 파일 만들기\n",
        "  import csv\n",
        "  if os.path.exists(filename):\n",
        "    os.remove(filename)\n",
        "  else:\n",
        "    print(\"Create New {} file.\".format(filename))\n",
        "\n",
        "  try:\n",
        "    with open (filename, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      for index, it in df.iterrows():\n",
        "        writer.writerow(it)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  sec = time.time() - Training_start\n",
        "  times = str(timedelta(seconds=sec)).split(\".\")\n",
        "  print('Making Time: %s' %times[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}